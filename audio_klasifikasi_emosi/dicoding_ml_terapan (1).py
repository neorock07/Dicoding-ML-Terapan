{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":639622,"sourceType":"datasetVersion","datasetId":316368},{"sourceId":9841524,"sourceType":"datasetVersion","datasetId":6037534},{"sourceId":9841546,"sourceType":"datasetVersion","datasetId":6037550},{"sourceId":9873546,"sourceType":"datasetVersion","datasetId":6061404},{"sourceId":9873567,"sourceType":"datasetVersion","datasetId":6061421}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# # Klasifikasi Emosi berdasarkan Suara Audio\n\n# %% [markdown]\n# Kode notebook ini dijalankan pada platform Kaggle, sehingga pada kode berikut digunakan untuk memuat dataset yang hendak dipakai pada proyek ini.\n\n# %% [markdown]\n# # Informasi Dataset :\n# \n# Nama : **Toronto emotional speech set (TESS)** \n# \r# URL  : [Dataset TESS](https://www.kaggle.com/datasets/ejlok1/toronto-emotional-speech-set-tess)\n# \n# \n# Terdapat satu set 200 kata target yang diucapkan dalam frasa \"Say the word _' oleh dua aktris (berusia 26 dan 64 tahun) dan rekaman dibuat dari set yang menggambarkan masing-masing dari tujuh emosi (marah, jijik, takut, bahagia, terkejut, sedih, dan netral). Ada 2800 data (file audio) secara total.\r# \n# \r# \n# Set data tersebut disusun sedemikian rupa sehingga masing-masing dari dua aktor wanita dan emosi mereka terkandung dalam foldernya sendiri. Dan di dalamnya, semua 200 file audio kata target dapat ditemukan. Format file audio adalah forma`WAV`.AV\n\n# %% [markdown]\n# # Pre-processing Dataset\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-11-11T12:17:47.320065Z\",\"iopub.execute_input\":\"2024-11-11T12:17:47.320831Z\",\"iopub.status.idle\":\"2024-11-11T12:17:47.675180Z\",\"shell.execute_reply.started\":\"2024-11-11T12:17:47.320789Z\",\"shell.execute_reply\":\"2024-11-11T12:17:47.674190Z\"}}\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\nROOT_DIR = \"/kaggle/input/toronto-emotional-speech-set-tess/TESS Toronto emotional speech set data/\"\nos.listdir(ROOT_DIR)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# **Contoh isi file**\n\n# %% [markdown]\n# kode ini digunakan untuk melihat sebagian isi folder YAF_fear.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-11-11T12:17:49.221826Z\",\"iopub.execute_input\":\"2024-11-11T12:17:49.222592Z\",\"iopub.status.idle\":\"2024-11-11T12:17:49.272246Z\",\"shell.execute_reply.started\":\"2024-11-11T12:17:49.222551Z\",\"shell.execute_reply\":\"2024-11-11T12:17:49.271329Z\"},\"jupyter\":{\"outputs_hidden\":false}}\nprint(os.listdir(ROOT_DIR+\"YAF_fear\")[:5])\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# **Jumlah file per folder emosi**\n# \n# Kode ini akan menghitung jumlah file yang ada di dalam setiap folder, kemudian menghitung total agregat data tersebut. Ini berguna untuk mengetahui distribusi jumlah file tiap folder dan untuk mengetahui `class` data apa saja yang ada pada dataset ini.\n# \n# sebagai catatan:\n# seluruh file audio ini bertipe `.wav`\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-11-11T12:17:50.361944Z\",\"iopub.execute_input\":\"2024-11-11T12:17:50.362782Z\",\"iopub.status.idle\":\"2024-11-11T12:17:50.902148Z\",\"shell.execute_reply.started\":\"2024-11-11T12:17:50.362740Z\",\"shell.execute_reply\":\"2024-11-11T12:17:50.901209Z\"},\"jupyter\":{\"outputs_hidden\":false}}\ncount = 0\nfor i in os.listdir(ROOT_DIR):\n    len_dir = len(os.listdir(ROOT_DIR+i)) \n    print(f\"Jumlah file DIR : {i} -> {len_dir}\")\n    count += len_dir\n\nprint(f\"Total : {count}\")\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# **play beberapa audio**\n# \n# Kode ini digunakan untuk memutar file audio dari beberapa folder yang berbeda, sebagai sample untuk mengetahui bunyi masing-masing audio tersebut.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-11-11T12:17:50.903906Z\",\"iopub.execute_input\":\"2024-11-11T12:17:50.904657Z\",\"iopub.status.idle\":\"2024-11-11T12:18:04.739753Z\",\"shell.execute_reply.started\":\"2024-11-11T12:17:50.904609Z\",\"shell.execute_reply\":\"2024-11-11T12:18:04.738754Z\"},\"jupyter\":{\"outputs_hidden\":false}}\nfrom IPython.display import Audio\nimport librosa\n\ny, sr = librosa.load(ROOT_DIR + \"YAF_fear/\" + os.listdir(ROOT_DIR+\"YAF_fear\")[10])\nAudio(data=y, rate=sr)\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# **contoh audio Angry**\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-11-11T12:18:04.741591Z\",\"iopub.execute_input\":\"2024-11-11T12:18:04.742021Z\",\"iopub.status.idle\":\"2024-11-11T12:18:04.761466Z\",\"shell.execute_reply.started\":\"2024-11-11T12:18:04.741986Z\",\"shell.execute_reply\":\"2024-11-11T12:18:04.760602Z\"}}\ny, sr = librosa.load(ROOT_DIR + \"OAF_angry/\" + os.listdir(ROOT_DIR+\"OAF_angry\")[10])\nAudio(data=y, rate=sr)\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# **Contoh audio OAF Fear**\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-11-11T12:18:04.762488Z\",\"iopub.execute_input\":\"2024-11-11T12:18:04.762748Z\",\"iopub.status.idle\":\"2024-11-11T12:18:04.785060Z\",\"shell.execute_reply.started\":\"2024-11-11T12:18:04.762718Z\",\"shell.execute_reply\":\"2024-11-11T12:18:04.784239Z\"}}\ny, sr = librosa.load(ROOT_DIR + \"OAF_Fear/\" + os.listdir(ROOT_DIR+\"OAF_Fear\")[10])\nAudio(data=y, rate=sr)\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# **Contoh audio OAF disgust**\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-11-11T12:18:04.787486Z\",\"iopub.execute_input\":\"2024-11-11T12:18:04.788094Z\",\"iopub.status.idle\":\"2024-11-11T12:18:04.807025Z\",\"shell.execute_reply.started\":\"2024-11-11T12:18:04.788049Z\",\"shell.execute_reply\":\"2024-11-11T12:18:04.806121Z\"}}\ny, sr = librosa.load(ROOT_DIR + \"OAF_disgust/\" + os.listdir(ROOT_DIR+\"OAF_disgust\")[10])\nAudio(data=y, rate=sr)\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# **Contoh audio OAF Neutral**\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-11-11T12:18:04.807998Z\",\"iopub.execute_input\":\"2024-11-11T12:18:04.808267Z\",\"iopub.status.idle\":\"2024-11-11T12:18:04.827155Z\",\"shell.execute_reply.started\":\"2024-11-11T12:18:04.808237Z\",\"shell.execute_reply\":\"2024-11-11T12:18:04.826338Z\"}}\ny, sr = librosa.load(ROOT_DIR + \"OAF_neutral/\" + os.listdir(ROOT_DIR+\"OAF_neutral\")[10])\nAudio(data=y, rate=sr)\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# **Contoh audio YAF Angry**\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-11-11T12:18:04.828185Z\",\"iopub.execute_input\":\"2024-11-11T12:18:04.828538Z\",\"iopub.status.idle\":\"2024-11-11T12:18:04.848239Z\",\"shell.execute_reply.started\":\"2024-11-11T12:18:04.828498Z\",\"shell.execute_reply\":\"2024-11-11T12:18:04.847412Z\"}}\ny, sr = librosa.load(ROOT_DIR + \"YAF_angry/\" + os.listdir(ROOT_DIR+\"YAF_angry\")[10])\nAudio(data=y, rate=sr)\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# **Contoh audio OAF Surprise**\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-11-11T12:18:04.849406Z\",\"iopub.execute_input\":\"2024-11-11T12:18:04.849772Z\",\"iopub.status.idle\":\"2024-11-11T12:18:04.872190Z\",\"shell.execute_reply.started\":\"2024-11-11T12:18:04.849730Z\",\"shell.execute_reply\":\"2024-11-11T12:18:04.871444Z\"}}\ny, sr = librosa.load(ROOT_DIR + \"OAF_Pleasant_surprise/\" + os.listdir(ROOT_DIR+\"OAF_Pleasant_surprise\")[11])\nAudio(data=y, rate=sr)\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# **Contoh audio OAF Sad**\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-11-11T12:18:04.873256Z\",\"iopub.execute_input\":\"2024-11-11T12:18:04.873545Z\",\"iopub.status.idle\":\"2024-11-11T12:18:04.893540Z\",\"shell.execute_reply.started\":\"2024-11-11T12:18:04.873515Z\",\"shell.execute_reply\":\"2024-11-11T12:18:04.892770Z\"}}\ny, sr = librosa.load(ROOT_DIR + \"OAF_Sad/\" + os.listdir(ROOT_DIR+\"OAF_Sad\")[11])\nAudio(data=y, rate=sr)\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# > **Data Preparation**\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# Sebelum data audio ini diproses lebih lanjut, saya mengorganisasikan setiap file ke direktori yang sesuai dengan jenis file-nya, karena terdapat 2 jenis tipe emosi yang berbeda folder misal: `YAF_fear` dan `OAF_fear` kedua isi folder tersebut sepertinya dilafalkan oleh orang yang berbeda, meskipun demikian isi folder tersebut tetaplah akan bertipe emosi `fear` sehingga alangkah baiknya saya menyalin kedua isi folder tersebut dan meletakkanya di direktori yang sama misal `Fear`. Hal ini akan dilakukan pada seluruh isi folder dengan tipe emosi yang sama.\n# \n# Untuk melakukan hal ini saya membuat sebuah function `copy_to_dir` untuk menyalin dan meletakan setiap file ke masing-masing direktori yang baru.\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-11-11T12:18:04.894666Z\",\"iopub.execute_input\":\"2024-11-11T12:18:04.894947Z\",\"iopub.status.idle\":\"2024-11-11T12:18:04.901561Z\",\"shell.execute_reply.started\":\"2024-11-11T12:18:04.894916Z\",\"shell.execute_reply\":\"2024-11-11T12:18:04.900629Z\"}}\nimport shutil \nimport matplotlib.pyplot as plt\n\n\"\"\"\n   function menggabungkan dahulu tipe emosi yang sama ke dalam satu folder.\n\"\"\"\n\ndef copy_to_dir(dir_ori, dir_dst=\"dataset_audio\"):\n    os.makedirs(dir_dst, exist_ok=True)\n    for filename in os.listdir(dir_ori):\n        file_path = os.path.join(dir_ori, filename)\n        if os.path.isfile(file_path):  # Cek kalo itu file, bukan sub-folder\n            shutil.copy(file_path, dir_dst) \n\n\"\"\"\n    membuat direktori baru untuk memindahkan file-file tersebut.\n\"\"\"\nif os.path.isdir(\"dataset_audio\"):\n    for i in os.listdir(ROOT_DIR):\n        print(os.path.join(ROOT_DIR, i))\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# **Memindahkan file ke direktori baru.**\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-11-11T12:18:04.905405Z\",\"iopub.execute_input\":\"2024-11-11T12:18:04.905705Z\",\"iopub.status.idle\":\"2024-11-11T12:18:06.089174Z\",\"shell.execute_reply.started\":\"2024-11-11T12:18:04.905674Z\",\"shell.execute_reply\":\"2024-11-11T12:18:06.088142Z\"}}\ncopy_to_dir(\"/kaggle/input/toronto-emotional-speech-set-tess/TESS Toronto emotional speech set data/YAF_fear\", \"dataset_audio/fear\")\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-11-11T12:18:06.090382Z\",\"iopub.execute_input\":\"2024-11-11T12:18:06.090678Z\",\"iopub.status.idle\":\"2024-11-11T12:18:23.700078Z\",\"shell.execute_reply.started\":\"2024-11-11T12:18:06.090647Z\",\"shell.execute_reply\":\"2024-11-11T12:18:23.699263Z\"}}\n\"\"\"\n    copy semua file ke folder yang baru.\n\"\"\"\n\ncopy_to_dir(\"/kaggle/input/toronto-emotional-speech-set-tess/TESS Toronto emotional speech set data/OAF_angry\", \"dataset_audio/angry\")\ncopy_to_dir(\"/kaggle/input/toronto-emotional-speech-set-tess/TESS Toronto emotional speech set data/OAF_Fear\", \"dataset_audio/fear\")\ncopy_to_dir(\"/kaggle/input/toronto-emotional-speech-set-tess/TESS Toronto emotional speech set data/OAF_disgust\", \"dataset_audio/disgust\")\ncopy_to_dir(\"/kaggle/input/toronto-emotional-speech-set-tess/TESS Toronto emotional speech set data/OAF_neutral\", \"dataset_audio/neutral\")\ncopy_to_dir(\"/kaggle/input/toronto-emotional-speech-set-tess/TESS Toronto emotional speech set data/YAF_angry\", \"dataset_audio/angry\")\ncopy_to_dir(\"/kaggle/input/toronto-emotional-speech-set-tess/TESS Toronto emotional speech set data/OAF_Sad\", \"dataset_audio/sad\")\ncopy_to_dir(\"/kaggle/input/toronto-emotional-speech-set-tess/TESS Toronto emotional speech set data/YAF_disgust\", \"dataset_audio/disgust\")\ncopy_to_dir(\"/kaggle/input/toronto-emotional-speech-set-tess/TESS Toronto emotional speech set data/YAF_neutral\", \"dataset_audio/neutral\")\ncopy_to_dir(\"/kaggle/input/toronto-emotional-speech-set-tess/TESS Toronto emotional speech set data/OAF_Pleasant_surprise\", \"dataset_audio/pleasant_surprise\")\ncopy_to_dir(\"/kaggle/input/toronto-emotional-speech-set-tess/TESS Toronto emotional speech set data/YAF_happy\", \"dataset_audio/happy\")\ncopy_to_dir(\"/kaggle/input/toronto-emotional-speech-set-tess/TESS Toronto emotional speech set data/OAF_happy\", \"dataset_audio/happy\")\ncopy_to_dir(\"/kaggle/input/toronto-emotional-speech-set-tess/TESS Toronto emotional speech set data/YAF_sad\", \"dataset_audio/sad\")\ncopy_to_dir(\"/kaggle/input/toronto-emotional-speech-set-tess/TESS Toronto emotional speech set data/YAF_pleasant_surprised\", \"dataset_audio/pleasant_surprise\")\n\n# %% [markdown]\n# Setelah menyalin semua audio ke direktori yang baru, yaitu : `dataset_audio`, maka direktori ini yang akan dijadikan ROOT direktori kedepannya.\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-11-11T12:18:23.701183Z\",\"iopub.execute_input\":\"2024-11-11T12:18:23.701472Z\",\"iopub.status.idle\":\"2024-11-11T12:18:23.708238Z\",\"shell.execute_reply.started\":\"2024-11-11T12:18:23.701440Z\",\"shell.execute_reply\":\"2024-11-11T12:18:23.707338Z\"}}\n\"\"\"\n    atur sebagai ROOT direktori yang baru.\n\"\"\"\n\nROOT_DIR = \"dataset_audio\"\n\nkelas =  os.listdir(ROOT_DIR)\nkelas\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# **Menyimpan file_path dan label ke list**\n\n# %% [markdown]\n# Selanjutnya saya akan memasangkan setiap file dengan label nya masing-masing, untuk itu saya akan menyimpan terlebih dahulu lokasi file `(file path)`  dan label-nya ke variabel `list` yang nantinya akan diubah ke bentuk DataFrame pandas.\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-11-11T12:18:23.709417Z\",\"iopub.execute_input\":\"2024-11-11T12:18:23.709706Z\",\"iopub.status.idle\":\"2024-11-11T12:18:23.728815Z\",\"shell.execute_reply.started\":\"2024-11-11T12:18:23.709674Z\",\"shell.execute_reply\":\"2024-11-11T12:18:23.727976Z\"}}\n\"\"\"\n    kode untuk menyimpan file-path dan label nya ke list.\n\"\"\"\n\naudio_paths = []\nlabels = []\n\nfor folder in os.listdir(ROOT_DIR):\n    folder_path = os.path.join(ROOT_DIR, folder)\n    if os.path.isdir(folder_path): \n        label = folder.split('_')[-1]  # Ambil emosi dari nama folder (contoh: \"YAF_fear\" -> \"fear\")\n        for filename in os.listdir(folder_path):\n            if filename.endswith(\".wav\"):\n                file_path = os.path.join(folder_path, filename)\n                audio_paths.append(file_path)\n                labels.append(label)\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# **Membuat DataFrame**\n\n# %% [markdown]\n# Hasil dari kode sebelumnya yaitu array-array list tersebut akan digunakan untuk membuat DataFrame pandas, agar kedepan mudah untuk dilakukan manipulasi data.\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-11-11T12:18:23.730022Z\",\"iopub.execute_input\":\"2024-11-11T12:18:23.730335Z\",\"iopub.status.idle\":\"2024-11-11T12:18:23.748168Z\",\"shell.execute_reply.started\":\"2024-11-11T12:18:23.730294Z\",\"shell.execute_reply\":\"2024-11-11T12:18:23.747303Z\"}}\n\"\"\"\n    membuat dataframe dengan kolom nama file_path dan label nya\n\"\"\"\n\n# Dataframe untuk organisasi data\ndata = pd.DataFrame({\n    'file_path': audio_paths,\n    'label': labels\n})\n\n# Cek data\nprint(f\"Total files: {len(data)}\")\ndata.head()\n\n# %% [markdown]\n# # Explorasi Data\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# **Data Exploration**\n\n# %% [markdown]\n# Untuk plot beberapa file sekaligus saya mengambil setiap satu file path dari semua kategori emosi dan disimpan ke sebuah list.\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-11-11T12:18:23.749511Z\",\"iopub.execute_input\":\"2024-11-11T12:18:23.750173Z\",\"iopub.status.idle\":\"2024-11-11T12:18:23.758946Z\",\"shell.execute_reply.started\":\"2024-11-11T12:18:23.750125Z\",\"shell.execute_reply\":\"2024-11-11T12:18:23.758056Z\"}}\n\"\"\"\n    mengambil contoh file audio untuk di plot.\n\"\"\"\n\ncontoh_file_path = [ os.path.join(ROOT_DIR, os.path.join(x, os.listdir(os.path.join(ROOT_DIR, x))[0]) )  for x in kelas ]\ncontoh_file_path\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# **Plot Gelombang Audio**\n\n# %% [markdown]\n# Kode dibawah digunakan untuk plot setiap file yang ada di list sebelumnya dengan menggunakan `plt.subplots` agar menggunakan mode Grid pada matplotlib, sehingga dapat melihat hasil grafik spektrum gelombang audio setiap file sample yang telah diambil tersebut.\n# \n# Grafik ini mengukur Amplitudo gelombang terhadap waktu (time) setiap audio jadi data ini masih dalam domain waktu, nantinya kita akan mengubah data ini ke domain frekuensi agar lebih banyak mendapatkan `insight`. dapat dilihat pada contoh data sample ini bahwa pada audio jenis neutral spektrum gelombang cenderung lebih stabil pada panjang amplitudo-nya, sedangkan pada kategori emosi lain bentuk spektrum gelombang lebih bervariasi.\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-11-11T12:18:23.760120Z\",\"iopub.execute_input\":\"2024-11-11T12:18:23.760770Z\",\"iopub.status.idle\":\"2024-11-11T12:18:31.634056Z\",\"shell.execute_reply.started\":\"2024-11-11T12:18:23.760722Z\",\"shell.execute_reply\":\"2024-11-11T12:18:31.633111Z\"}}\nimport librosa.display\n\n\"\"\"\n    plot data gelombang sinyal audio\n\"\"\"\n\nfig, axes = plt.subplots(2, 4, figsize=(20, 10))\naxes = axes.flatten()\n\nfor i, file_path in enumerate(contoh_file_path):\n    y, sr = librosa.load(file_path)    \n    # Plot wave plot di subplot\n    librosa.display.waveshow(y, sr=sr, ax=axes[i])\n    # Nama file sebagai judul\n    axes[i].set_title(file_path.split('/')[-1])  \n    axes[i].set_xlabel(\"Time (s)\")\n    axes[i].set_ylabel(\"Amplitude\")\n\n# Hapus subplot kosong\nfor j in range(i + 1, len(axes)):\n    fig.delaxes(axes[j])  \n\nplt.tight_layout()\nplt.show()\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# **Plot MFCC (Mel Frequency Cepstral Coefficient) Spectogram**\n\n# %% [markdown]\n# Untuk mengolah data sinyak audio ini kita perlu mengubah data yang masih dalam domain waktu untuk menjadi data dalam domain frekuensi, alasannya karena kita dapat mudah menemukan pola-pola frekuensi yang berkarakter, ini sama halnya dengan cara telinga kita mendengar yaitu kita akan peka terhadap bermacam-macam pola frekuensi dari rambatan suara tersebut ketimbang memikirkan amplitudio sinyal suara tiap waktu.\n# \n# `MFCC` (**Mel Frequency Cepstral Coefficient**) merupakan salah satu bentuk data sinyal audio dalam bentuk domain frekuensi dengan rentang skala Mel, skala Mel ini adalah skala frekuensi yang didasarkan pada cara manusia mendengar suara.\n# \n# Kode ini akan menampilkan grafik map dari MFCC tiap sample file audio. koefisien MFCC yang diambil berjumlah 13 koefisien karena biasanya koefisien lebih dari 13 adalah sebuah noise, pada grafik tersebut juga terlihat rentang nilai desible antara 100 db - 600 db.\n# \n# \n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-11-11T12:18:31.635339Z\",\"iopub.execute_input\":\"2024-11-11T12:18:31.635717Z\",\"iopub.status.idle\":\"2024-11-11T12:18:34.749933Z\",\"shell.execute_reply.started\":\"2024-11-11T12:18:31.635672Z\",\"shell.execute_reply\":\"2024-11-11T12:18:34.748823Z\"}}\nfig, axes = plt.subplots(2, 4, figsize=(20, 10))\naxes = axes.flatten()  \n\nfor i, file_path in enumerate(contoh_file_path):\n    # Load audio file\n    y, sr = librosa.load(file_path)\n    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n    # Plot MFCC Spectogram\n    img = librosa.display.specshow(mfccs, x_axis='time', sr=sr, cmap='viridis', ax=axes[i])\n    fig.colorbar(img, ax=axes[i], format='%+2.0f dB')\n    axes[i].set_title(\"MFCC Spectrogram \" +  file_path.split('/')[-1])\n    axes[i].set_xlabel(\"Time (s)\")\n    axes[i].set_ylabel(\"MFCC Coefficients\")\n\n# Hapus subplot kosong\nfor j in range(i + 1, len(axes)):\n    fig.delaxes(axes[j])  \n\nplt.tight_layout()\nplt.show()\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# **Melihat Distribusi Durasi Audio**\n\n# %% [markdown]\n# Pada bagian ini saya ingin melihat distribusi durasi audio pada dataset, dapat dilihat distribusi audio berdistribusi normal, dengan rentang durasi paling pendek yaitu 1.2 detik dan paling panjang 3 detik.\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-11-11T12:18:34.751460Z\",\"iopub.execute_input\":\"2024-11-11T12:18:34.752264Z\",\"iopub.status.idle\":\"2024-11-11T12:18:40.568229Z\",\"shell.execute_reply.started\":\"2024-11-11T12:18:34.752215Z\",\"shell.execute_reply\":\"2024-11-11T12:18:40.567319Z\"}}\nimport seaborn as sns\n\ndurations = []\nfor i, file_path in enumerate(data.file_path):\n    y, sr = librosa.load(file_path)\n    durations.append(librosa.get_duration(y=y, sr=sr))\n\nsns.histplot(durations, kde=True, color='red')\nplt.xlabel(\"Durasi (s)\")\nplt.ylabel(\"Jumlah Data\")\nplt.title(\"Distribusi Durasi Audio\")\nplt.show()\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# **Melihat Distribusi Roll-Off Audio**\n# Untuk mengetahui dimana sebagian besar energi berada\n\n# %% [markdown]\n# Eksplorasi selanjutnya yaitu untuk melihat distribusi data `Spectral Roll-Off` ini merupakan data yang mengindikasikan frekuensi dengan energi nya 95% di bawah normal.\n# dapat dilihat data ini juga terdistribusi normal, dengan jumlah data Roll-off terbanyak diantara 4000-5000 Hz.\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-11-11T12:18:40.569475Z\",\"iopub.execute_input\":\"2024-11-11T12:18:40.569911Z\",\"iopub.status.idle\":\"2024-11-11T12:18:54.226755Z\",\"shell.execute_reply.started\":\"2024-11-11T12:18:40.569876Z\",\"shell.execute_reply\":\"2024-11-11T12:18:54.225817Z\"}}\nroll_off = []\nfor i, file_path in enumerate(data.file_path):\n    # Load audio file\n    y, sr = librosa.load(file_path)\n    roll_off.append(librosa.feature.spectral_rolloff(y=y, sr=sr).mean())\n\nsns.histplot(roll_off, kde=True, color='violet')\nplt.xlabel(\"Roll-Off (Hz)\")\nplt.ylabel(\"Jumlah Data\")\nplt.title(\"Distribusi Spectral Roll-Off\")\nplt.show()\n\n# %% [markdown]\n# Selanjutnya kita akan melihat data tentang tingkat `Zero Crossing Rate`, yaitu seberapa banyak sinyal audio melewati nilai 0, misalkan dari tingkat positif ke nol lalu ke negatif dan juga sebaliknya. semakin banyak nilai ZCR ini berarti tingkat frekuensi juga tinggi, suara ber-intonasi dan tekanan tinggi, dan sebaliknya\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-11-11T12:18:54.228108Z\",\"iopub.execute_input\":\"2024-11-11T12:18:54.228519Z\",\"iopub.status.idle\":\"2024-11-11T12:19:02.412725Z\",\"shell.execute_reply.started\":\"2024-11-11T12:18:54.228474Z\",\"shell.execute_reply\":\"2024-11-11T12:19:02.411736Z\"}}\nz_cross = []\nfor i, file_path in enumerate(data.file_path):\n    y, sr = librosa.load(file_path)\n    zcr = librosa.feature.zero_crossing_rate(y)\n    z_cross.append(zcr.mean())\n\nsns.histplot(z_cross, kde=True, color='orange')\nplt.xlabel(\"Zero Crossing Rate\")\nplt.ylabel(\"Jumlah Data\")\nplt.title(\"Zero Crossing Rate Distribution\")\nplt.show()\n\n# %% [markdown]\n# Kode ini digunakan untuk melihat distribusi pusat massa sinyal atau `Spectral Centroid`, nilai ini adalah frekuensi rata-rata sinyal yang mengindikasikan karakteristik suara yaitu bernada tinggi atau rendah. \n# \n# Pada grafik di bawah dapat terlihat mayoritas pusat massa audio pada dataset ini pada frekuensi antara 2000-2500 Hz.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-11-11T12:19:02.413837Z\",\"iopub.execute_input\":\"2024-11-11T12:19:02.414147Z\",\"iopub.status.idle\":\"2024-11-11T12:19:15.779104Z\",\"shell.execute_reply.started\":\"2024-11-11T12:19:02.414113Z\",\"shell.execute_reply\":\"2024-11-11T12:19:15.778189Z\"}}\ncentroid = []\nfor i, file_path in enumerate(data.file_path):\n    # Load audio file\n    y, sr = librosa.load(file_path)\n    centroid.append(librosa.feature.spectral_centroid(y=y, sr=sr).mean())\n\nsns.histplot(centroid, kde=True, color='blue')\nplt.xlabel(\"Spectral Centroid (Hz)\")\nplt.ylabel(\"Jumlah Data\")\nplt.title(\"Distribusi Spectral Centroid\")\nplt.show()\n\n# %% [markdown]\n# `Spectral Contrast` merupakan data tekait perbedaan energi frekuensi tinggi dan rendah pada sinyal suara. pada grafik terlihat berdistribusi normal, dengan nilai selisih paling banyak ada pada rentang 24-26.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-11-11T12:19:15.783032Z\",\"iopub.execute_input\":\"2024-11-11T12:19:15.783354Z\",\"iopub.status.idle\":\"2024-11-11T12:19:31.596141Z\",\"shell.execute_reply.started\":\"2024-11-11T12:19:15.783320Z\",\"shell.execute_reply\":\"2024-11-11T12:19:31.595133Z\"}}\ncontrast = []\nfor i, file_path in enumerate(data.file_path):\n    # Load audio file\n    y, sr = librosa.load(file_path)\n    contrast.append(librosa.feature.spectral_contrast(y=y, sr=sr).mean())\n\nsns.histplot(contrast, kde=True, color='green')\nplt.xlabel(\"Spectral Contrast\")\nplt.ylabel(\"Jumlah Data\")\nplt.title(\"Distribusi Spectral Contrast\")\nplt.show()\n\n# %% [markdown]\n# Pada bagian ini digunakan untuk melihat tingkat distribusi `Spectral Bandwidth` atau lebar rentang frekuensi sinyal audio. pada grafik di bawah terdistribusi normal dengan titik data terbanyak pada rentang frekuensi 2200 Hz.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-11-11T12:55:56.111207Z\",\"iopub.execute_input\":\"2024-11-11T12:55:56.112236Z\",\"iopub.status.idle\":\"2024-11-11T12:56:12.016784Z\",\"shell.execute_reply.started\":\"2024-11-11T12:55:56.112193Z\",\"shell.execute_reply\":\"2024-11-11T12:56:12.015757Z\"}}\nbandwidth = []\nfor i, file_path in enumerate(data.file_path):\n    # Load audio file\n    y, sr = librosa.load(file_path)\n    bandwidth.append(librosa.feature.spectral_bandwidth(y=y, sr=sr).mean())\n\nsns.histplot(bandwidth, kde=True, color='brown')\nplt.xlabel(\"Spectral Bandwidth (Hz)\")\nplt.ylabel(\"Jumlah Data\")\nplt.title(\"Distribusi Spectral Bandwidth\")\nplt.show()\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2024-11-11T12:56:24.940549Z\",\"iopub.execute_input\":\"2024-11-11T12:56:24.941418Z\",\"iopub.status.idle\":\"2024-11-11T12:56:25.252473Z\",\"shell.execute_reply.started\":\"2024-11-11T12:56:24.941377Z\",\"shell.execute_reply\":\"2024-11-11T12:56:25.251442Z\"}}\nlabel_counts = [len(os.listdir(os.path.join(ROOT_DIR, x))) for x in kelas]\nplt.figure(figsize=(11, 10))\nsns.barplot(x=kelas, y=label_counts)\nplt.title(\"Label Distribution\")\nplt.xlabel(\"Emotion\")\nplt.ylabel(\"Count\")\nplt.show()\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# # Feature Engineering\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# Pada kode di bawah adalah fungsi `extract_feature` untuk mengekstrak nilai berikut:\n# \n# \n# 1. rata-rata 13 koefisien MFCC\n# 2. rata-rata spectral centroid\n# 3. rata-rata spectral contrast\n# 4. rata-rata spectral bandwidth\n# 5. mean zero crossing rate\n# 6. mean spectral roll off \n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-11-11T13:05:37.359305Z\",\"iopub.execute_input\":\"2024-11-11T13:05:37.360308Z\",\"iopub.status.idle\":\"2024-11-11T13:05:37.367532Z\",\"shell.execute_reply.started\":\"2024-11-11T13:05:37.360242Z\",\"shell.execute_reply\":\"2024-11-11T13:05:37.366503Z\"},\"jupyter\":{\"outputs_hidden\":false}}\ndef extract_feature(file_path):\n    y, sr = librosa.load(file_path)\n    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n    mfcc_mean = mfcc.mean(axis=1)\n    zcr = librosa.feature.zero_crossing_rate(y=y).mean()\n    roll_off = librosa.feature.spectral_rolloff(y=y, sr=sr).mean()\n    spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr).mean()\n    spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr).mean()\n    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr).mean()\n\n    features = mfcc_mean.tolist() + [zcr, roll_off, spectral_centroid, spectral_contrast, spectral_bandwidth]\n    return features\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-11-11T13:06:22.587482Z\",\"iopub.execute_input\":\"2024-11-11T13:06:22.588115Z\",\"iopub.status.idle\":\"2024-11-11T13:06:22.640813Z\",\"shell.execute_reply.started\":\"2024-11-11T13:06:22.588072Z\",\"shell.execute_reply\":\"2024-11-11T13:06:22.639512Z\"},\"jupyter\":{\"outputs_hidden\":false}}\n\"\"\"\n    contoh hasil ekstraksi fitur memakai fungsi tersebut.\n\"\"\"\ncontoh = extract_feature(data.file_path[0])\nprint(contoh)\nlen(contoh)\n\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# Kode di bawah digunakan untuk menjalankan operasi fungsi `extract_feature` ke semua data kolom file_path Data Frame dan kemudian menyimpannya menjadi kolom-kolom/feature sesuai yang telah di-ekstrak.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-11-11T13:09:57.619510Z\",\"iopub.execute_input\":\"2024-11-11T13:09:57.620235Z\",\"iopub.status.idle\":\"2024-11-11T13:11:56.399397Z\",\"shell.execute_reply.started\":\"2024-11-11T13:09:57.620195Z\",\"shell.execute_reply\":\"2024-11-11T13:11:56.397922Z\"},\"jupyter\":{\"outputs_hidden\":false}}\nfeatures = data['file_path'].apply(lambda x : extract_feature(x))\nkolom_features = pd.DataFrame(features.tolist())\nkolom_features.columns = [f'mfcc_{i}' for i in range(13)] + ['zcr', 'spectral_rolloff', 'spectral_centroid', 'spectral_bandwidth', 'spectral_contrast']\ndata = pd.concat([data, kolom_features], axis=1)\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# kode untuk menampilkan data frame, dapat dilihat terdapat 18 kolom pada dataframe, tapi yang akan digunakan kolom kecuali kolom `file_path`. \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-11-11T13:12:47.379214Z\",\"iopub.execute_input\":\"2024-11-11T13:12:47.379644Z\",\"iopub.status.idle\":\"2024-11-11T13:12:47.402142Z\",\"shell.execute_reply.started\":\"2024-11-11T13:12:47.379603Z\",\"shell.execute_reply\":\"2024-11-11T13:12:47.401265Z\"},\"jupyter\":{\"outputs_hidden\":false}}\ndata.head()\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# **Melihat statistik deskriptif dataset.**\n# \n# pada tabel di bawah dapat dilihat nilai mean tiap kolom, standar deviasi, nilai maximal, minimal serta quartil data.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-11-11T13:17:30.361170Z\",\"iopub.execute_input\":\"2024-11-11T13:17:30.361597Z\",\"iopub.status.idle\":\"2024-11-11T13:17:30.416228Z\",\"shell.execute_reply.started\":\"2024-11-11T13:17:30.361559Z\",\"shell.execute_reply\":\"2024-11-11T13:17:30.415332Z\"},\"jupyter\":{\"outputs_hidden\":false}}\ndata.describe()\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# **Matriks Korelasi antar Fitur**\n# \n# Pada grafik heatmap di bawah merupakan korelasi antar fitur/kolom dataset, dapat diketahui bahwa fitur zero crossing rate, spectral roll off, dan spectral centroid berkorelasi positif artinya ketiganya akan saling berpengaruh secara linear, jadi jika salah satu bernilai tinggi maka kedua fitur lain itu juga akan sama-sama bernilai tinggi dan sebaliknya.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-11-11T13:17:36.688120Z\",\"iopub.execute_input\":\"2024-11-11T13:17:36.688758Z\",\"iopub.status.idle\":\"2024-11-11T13:17:37.748074Z\",\"shell.execute_reply.started\":\"2024-11-11T13:17:36.688718Z\",\"shell.execute_reply\":\"2024-11-11T13:17:37.747146Z\"},\"jupyter\":{\"outputs_hidden\":false}}\ndata_to_map = data.drop(['file_path', 'label'], axis=1)\nplt.figure(figsize=(12, 10))\nsns.heatmap(data_to_map.corr(), annot=True, cmap=\"coolwarm\")\nplt.title(\"Feature Correlation Matrix\")\nplt.show()\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# **Mengubah fitur label yang bertipe kategorik menjadi data numerik nominal.**\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-11-11T13:24:58.857783Z\",\"iopub.execute_input\":\"2024-11-11T13:24:58.858193Z\",\"iopub.status.idle\":\"2024-11-11T13:24:58.873506Z\",\"shell.execute_reply.started\":\"2024-11-11T13:24:58.858154Z\",\"shell.execute_reply\":\"2024-11-11T13:24:58.872550Z\"},\"jupyter\":{\"outputs_hidden\":false}}\nkelas_num = [x for x in range(0, 7)]\nkelas = data['label'].unique()\ndata['label'] = data['label'].replace(to_replace=kelas, value=kelas_num)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-11-11T13:25:15.537119Z\",\"iopub.execute_input\":\"2024-11-11T13:25:15.537771Z\",\"iopub.status.idle\":\"2024-11-11T13:25:15.558840Z\",\"shell.execute_reply.started\":\"2024-11-11T13:25:15.537728Z\",\"shell.execute_reply\":\"2024-11-11T13:25:15.557923Z\"},\"jupyter\":{\"outputs_hidden\":false}}\ndata.head()\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# Membagi data untuk persiapan training, dengan data X untuk kolom-kolom yang digunakan training sedangkan Y sebagai label.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-11-11T13:25:31.006540Z\",\"iopub.execute_input\":\"2024-11-11T13:25:31.007190Z\",\"iopub.status.idle\":\"2024-11-11T13:25:31.012736Z\",\"shell.execute_reply.started\":\"2024-11-11T13:25:31.007146Z\",\"shell.execute_reply\":\"2024-11-11T13:25:31.011504Z\"},\"jupyter\":{\"outputs_hidden\":false}}\ny = data['label']\nX = data.drop(['file_path', 'label'], axis=1)\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# Normalisasi data X (training) agar meringankan proses komputasi dan menjaga ke-stabilan range antar data.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-11-11T13:26:40.236171Z\",\"iopub.execute_input\":\"2024-11-11T13:26:40.237062Z\",\"iopub.status.idle\":\"2024-11-11T13:26:40.287780Z\",\"shell.execute_reply.started\":\"2024-11-11T13:26:40.237019Z\",\"shell.execute_reply\":\"2024-11-11T13:26:40.286985Z\"},\"jupyter\":{\"outputs_hidden\":false}}\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nstandardized_data = scaler.fit_transform(X)\nX = pd.DataFrame(standardized_data, columns=X.columns)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-11-11T13:26:45.087642Z\",\"iopub.execute_input\":\"2024-11-11T13:26:45.088004Z\",\"iopub.status.idle\":\"2024-11-11T13:26:45.108022Z\",\"shell.execute_reply.started\":\"2024-11-11T13:26:45.087971Z\",\"shell.execute_reply\":\"2024-11-11T13:26:45.107020Z\"},\"jupyter\":{\"outputs_hidden\":false}}\nX.head()\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# **Split dataset.**\n# \n# pada bagian ini digunakan untuk membagi data untuk training dan testing dengan perbandingan 80:20.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-11-11T13:29:35.983600Z\",\"iopub.execute_input\":\"2024-11-11T13:29:35.983974Z\",\"iopub.status.idle\":\"2024-11-11T13:29:36.246194Z\",\"shell.execute_reply.started\":\"2024-11-11T13:29:35.983939Z\",\"shell.execute_reply\":\"2024-11-11T13:29:36.245429Z\"},\"jupyter\":{\"outputs_hidden\":false}}\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-11-11T13:29:39.872617Z\",\"iopub.execute_input\":\"2024-11-11T13:29:39.872980Z\",\"iopub.status.idle\":\"2024-11-11T13:29:39.894487Z\",\"shell.execute_reply.started\":\"2024-11-11T13:29:39.872945Z\",\"shell.execute_reply\":\"2024-11-11T13:29:39.893467Z\"},\"jupyter\":{\"outputs_hidden\":false}}\nx_train.head()\n\n# %% [markdown]\n# # Build Model\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# **Membuat model memakai Random Forest**\n# \n# Pada bagian ini kita akan melatih model Random Forest pada set data train, tanpa menggunakan tuning hyperparameter.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-11-11T13:31:11.120629Z\",\"iopub.execute_input\":\"2024-11-11T13:31:11.120994Z\",\"iopub.status.idle\":\"2024-11-11T13:31:12.064088Z\",\"shell.execute_reply.started\":\"2024-11-11T13:31:11.120961Z\",\"shell.execute_reply\":\"2024-11-11T13:31:12.063131Z\"},\"jupyter\":{\"outputs_hidden\":false}}\nfrom sklearn.ensemble import RandomForestClassifier\n\nrand_forest = RandomForestClassifier()\n\nrand_forest.fit(x_train, y_train)\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# **Cek score model random forest**\n# \n# setelah melatih model maka sekarang cek score akurasi model tersebut.\n\n# %% [markdown]\n# # Evaluasi model: Random Forest\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-11-11T13:31:16.432445Z\",\"iopub.execute_input\":\"2024-11-11T13:31:16.433331Z\",\"iopub.status.idle\":\"2024-11-11T13:31:16.457580Z\",\"shell.execute_reply.started\":\"2024-11-11T13:31:16.433267Z\",\"shell.execute_reply\":\"2024-11-11T13:31:16.456675Z\"},\"jupyter\":{\"outputs_hidden\":false}}\nrand_forest.score(x_test, y_test)\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# **Confusion Matriks untuk performa Random Forest terhadap data test**\n# \n# Agar kita dapat mengetahui secara lebih jelas mengenai performa model Random forest, kita dapat menggunakan confusion matriks guna melihat data selain akurasi semisal f1-score, recall, dan precision.\n# \n# hasil performa model dapat disimpulkan bahwa :\n# \n# \n# 1. Model ini punya akurasi 96%, artinya 96% dari total prediksi untuk semua kelas benar.\n# 2. Precision dan Recall rata-rata di setiap kelas sekitar 96% atau lebih tinggi, menunjukkan model cukup baik dalam mengenali setiap emosi dengan baik, dengan tingkat kesalahan yang rendah.\n# 3. F1-score juga konsisten tinggi untuk setiap emosi, khususnya untuk kelas seperti fear, angry, dan sad, yang mencapai hampir 98-99%, menunjukkan keseimbangan baik antara precision dan recall.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-11-11T13:34:42.221240Z\",\"iopub.execute_input\":\"2024-11-11T13:34:42.221660Z\",\"iopub.status.idle\":\"2024-11-11T13:34:42.591888Z\",\"shell.execute_reply.started\":\"2024-11-11T13:34:42.221620Z\",\"shell.execute_reply\":\"2024-11-11T13:34:42.590894Z\"},\"jupyter\":{\"outputs_hidden\":false}}\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Prediksi hasil tes\ny_pred = rand_forest.predict(x_test)\n\n# Classification Report\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred,  target_names=kelas))\n\n# Confusion Matrix\nprint(\"Confusion Matrix:\")\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=kelas,\n            yticklabels=kelas)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# **Fungsi untuk pre-processing data test. (misal uji coba dengan data secara individu atau batch)**\n# \n# yang dilakukan pada fungsi ini yaitu : \n# 1. load audio dari file path file yang hendak jadi uji coba\n# 2. menampilkan spektrum gelombang dalam domain waktu\n# 3. mengekstrak fitur sesuai dengan data training dengan fungsi `extract_feature`.\n# 4. normalisasi hasil ekstraksi fitur dengan objek standar scaler yang sudah dilatih pada data training.\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-11-11T13:42:47.372480Z\",\"iopub.execute_input\":\"2024-11-11T13:42:47.373505Z\",\"iopub.status.idle\":\"2024-11-11T13:42:47.379236Z\",\"shell.execute_reply.started\":\"2024-11-11T13:42:47.373452Z\",\"shell.execute_reply\":\"2024-11-11T13:42:47.378351Z\"},\"jupyter\":{\"outputs_hidden\":false}}\n\"\"\"\n    Fungsi untuk pre-processing data testing seblum dilakukan prediksi dengan model.\n\"\"\"\n\ndef load_test(file_path):\n    #load audio file\n    y, sr = librosa.load(file_path)\n    #membuat plot gelombang sinyal \n    librosa.display.waveshow(y, sr=sr)\n    #mengektrak fitur\n    features = extract_feature(file_path)\n    #normalisasi data fitur\n    norm = scaler.transform([features])\n    return norm\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# **Uji coba satu data uji**\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-11-11T13:42:52.263570Z\",\"iopub.execute_input\":\"2024-11-11T13:42:52.263934Z\",\"iopub.status.idle\":\"2024-11-11T13:42:52.883504Z\",\"shell.execute_reply.started\":\"2024-11-11T13:42:52.263900Z\",\"shell.execute_reply\":\"2024-11-11T13:42:52.882632Z\"},\"jupyter\":{\"outputs_hidden\":false}}\ncoba = load_test(data['file_path'][2016])\n\nprint(f\"{data['label'][2016]} : {kelas[data['label'][2016]]}\")\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# **Hasil Prediksi Random Forest**\n# \n# kemudian kita akan memprediksi data sample tersebut. pada contoh ini model dapat berhasil memprediksi dengan benar yaitu label `sad`\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-11-11T13:42:57.891678Z\",\"iopub.execute_input\":\"2024-11-11T13:42:57.892042Z\",\"iopub.status.idle\":\"2024-11-11T13:42:57.911984Z\",\"shell.execute_reply.started\":\"2024-11-11T13:42:57.892009Z\",\"shell.execute_reply\":\"2024-11-11T13:42:57.910287Z\"},\"jupyter\":{\"outputs_hidden\":false}}\n\"\"\"\n    hasil prediksi benar sesuai label data yang sebenarnya.\n\"\"\"\n\npred = rand_forest.predict(coba)\nkelas[pred]\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-11-11T13:46:55.037702Z\",\"iopub.execute_input\":\"2024-11-11T13:46:55.038355Z\",\"iopub.status.idle\":\"2024-11-11T13:46:55.057731Z\",\"shell.execute_reply.started\":\"2024-11-11T13:46:55.038314Z\",\"shell.execute_reply\":\"2024-11-11T13:46:55.056920Z\"}}\nimport pickle\n\n# Simpan model ke dalam file\nwith open('random_forest_model.pkl', 'wb') as file:\n    pickle.dump(rand_forest, file)\n\n# %% [markdown]\n# # Build Model : Dense Neural Network\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# **Membuat model Neural Network DNN (Dense Neural Network) dengan Hyperparameter Tuning**\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# Fungsi untuk membuat model DNN untuk di-`tuning` dengan Bayesian Opt. Neural network ini menerima input shape dengan besar (18,) sesuai dengan jumlah kolom data frame kita.\n# \n# variabel params nanti akan diisi dengan hasil tuning dan kemudian di-assign ke masing\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-11-11T14:02:17.889983Z\",\"iopub.execute_input\":\"2024-11-11T14:02:17.890641Z\",\"iopub.status.idle\":\"2024-11-11T14:02:17.898290Z\",\"shell.execute_reply.started\":\"2024-11-11T14:02:17.890598Z\",\"shell.execute_reply\":\"2024-11-11T14:02:17.897324Z\"},\"jupyter\":{\"outputs_hidden\":false}}\n\"\"\"\n    membuat template model DNN yang akan dicari hyperparameter nya dengan Bayesian Optimisation.\n\"\"\"\n\ndef create_model(params):\n    inputs = tf.keras.layers.Input(shape=(18,))\n    x = tf.keras.layers.Dense(int(params['unitA']), activation=\"relu\")(inputs)\n    x = tf.keras.layers.Dense(int(params['unitB']), activation=\"relu\")(x)\n    x = tf.keras.layers.Dense(int(params['unitC']), activation=\"relu\")(x)\n    x = tf.keras.layers.Dense(int(params['unitD']), activation=\"relu\")(x)\n    outputs = tf.keras.layers.Dense(7, activation=\"softmax\")(x)\n    \n    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"model_bento\")\n    optimizer = tf.keras.optimizers.Adam(learning_rate=params['learning_rate'])\n    model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n    \n    return model\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# **Fungsi untuk mencari Hyperparameter dengan Bayesian Optimisation.**\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-11-11T14:02:21.070393Z\",\"iopub.execute_input\":\"2024-11-11T14:02:21.071329Z\",\"iopub.status.idle\":\"2024-11-11T14:02:21.082583Z\",\"shell.execute_reply.started\":\"2024-11-11T14:02:21.071285Z\",\"shell.execute_reply\":\"2024-11-11T14:02:21.079535Z\"},\"jupyter\":{\"outputs_hidden\":false}}\nfrom hyperopt import fmin, tpe, hp, Trials\nfrom tensorflow.keras.callbacks import EarlyStopping\n\ncallback = EarlyStopping(patience=3, monitor=\"val_accuracy\", mode=\"max\")\n\n\"\"\"\n    Fungsi untuk mencari hyperparameter optimisation\n\"\"\"\ndef hyperparameter_tuning(params):\n    #panggil fungsi untuk membuat model.\n    model = create_model(params)\n    #jalankan training model\n    history = model.fit(x_train, y_train, validation_data=(x_test, y_test),\n                        epochs=20, batch_size=int(params['batch_size']), callbacks=[callback], verbose=0)\n    #ambil val_acc sebagai indikator\n    val_acc = history.history['val_accuracy'][-1]\n    #dikali -1 agar memaksimalkan val_accuracy\n    return -val_acc\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# **lakukan iterasi pencarian hyperparameter yang paling optimal.**\n# \n# Pada bagian mulai untuk pencarian hyperparameter yaitu jumlah unit tiap layer, learning rate dan batch_size. nilai ini akan dicari pada ruang pencarian (`space`), hasil dari iterasi pencarian ini yaitu nilai tiap hyperparameter dengan tingkat `val_accuracy` yang paling tinggi.\n\n# %% [markdown]\n# # Hyperparameter Tuning DNN\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-11-11T14:02:24.391929Z\",\"iopub.execute_input\":\"2024-11-11T14:02:24.392621Z\",\"iopub.status.idle\":\"2024-11-11T14:06:16.772605Z\",\"shell.execute_reply.started\":\"2024-11-11T14:02:24.392579Z\",\"shell.execute_reply\":\"2024-11-11T14:06:16.771664Z\"},\"jupyter\":{\"outputs_hidden\":false}}\nimport tensorflow as tf\n\n\"\"\"\n    ruang pencarian hyperparameter.\n\"\"\"\n\nspace = {\n    'unitA': hp.quniform('unitA', 16, 128, 1),\n    'unitB': hp.quniform('unitB', 16, 128, 1),\n    'unitC': hp.quniform('unitC', 16, 128, 1),\n    'unitD': hp.quniform('unitD', 16, 128, 1),\n    'learning_rate': hp.loguniform('learning_rate', -5, -2),  # Range antara sekitar 0.0001 dan 0.1\n    'batch_size': hp.quniform('batch_size', 16, 64, 1),\n}\n\n# Menyimpan hasil tiap iterasi\ntrials = Trials()\n\n# Menjalankan optimasi\nbest = fmin(fn=hyperparameter_tuning, space=space, algo=tpe.suggest, max_evals=30, trials=trials)\n\nprint(\"Best hyperparameters:\", best)\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# **Train model DNN dengan hyperparameter yang telah di-`tuning`**\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-11-11T14:06:30.522390Z\",\"iopub.execute_input\":\"2024-11-11T14:06:30.523297Z\",\"iopub.status.idle\":\"2024-11-11T14:06:38.073396Z\",\"shell.execute_reply.started\":\"2024-11-11T14:06:30.523233Z\",\"shell.execute_reply\":\"2024-11-11T14:06:38.072354Z\"},\"jupyter\":{\"outputs_hidden\":false}}\nmodel = create_model(best)\nmodel.fit(x_train, y_train, validation_data=(x_test, y_test),\n                        epochs=20, batch_size=int(best['batch_size']), callbacks=[callback], verbose=1)\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# **Uji Coba dengan data uji**\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-11-11T14:24:34.329815Z\",\"iopub.execute_input\":\"2024-11-11T14:24:34.330503Z\",\"iopub.status.idle\":\"2024-11-11T14:24:35.097184Z\",\"shell.execute_reply.started\":\"2024-11-11T14:24:34.330458Z\",\"shell.execute_reply\":\"2024-11-11T14:24:35.096233Z\"},\"jupyter\":{\"outputs_hidden\":false}}\ntest_nn = load_test(data['file_path'][2450])\npred_nn = model.predict(test_nn)\nprint(f\"{data['label'][2450]} : {kelas[data['label'][2450]]}\")\nprint(kelas[np.argmax(pred_nn)])\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# **Menampilkan hasil confusion matriks DNN terhadap data test.**\n\n# %% [markdown]\n# \n# 1. Model mencapai akurasi sebesar 0.97 atau 97%, menunjukkan performa yang sangat baik pada seluruh kelas emosi.\n# 2. Precision: Semua kelas memiliki precision di atas 0.94, artinya model cukup akurat dalam mengidentifikasi setiap emosi tanpa banyak kesalahan prediksi ke kelas lain.\n# 3. Recall: Recall untuk semua kelas juga di atas 0.92, menunjukkan bahwa model mampu menangkap sebagian besar instance untuk setiap kelas.\n# 4. F1-Score: Dengan nilai F1-score antara 0.94 hingga 0.99, model memiliki keseimbangan yang baik antara precision dan recall di semua kelas.\n# \n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-11-11T14:06:57.132554Z\",\"iopub.execute_input\":\"2024-11-11T14:06:57.133595Z\",\"iopub.status.idle\":\"2024-11-11T14:06:58.023870Z\",\"shell.execute_reply.started\":\"2024-11-11T14:06:57.133551Z\",\"shell.execute_reply\":\"2024-11-11T14:06:58.022911Z\"},\"jupyter\":{\"outputs_hidden\":false}}\nfrom sklearn.metrics import classification_report\nimport numpy as np\n\n# Melakukan prediksi pada data uji\ny_pred_proba = model.predict(x_test)  # Menghasilkan probabilitas untuk setiap kelas\ny_pred = np.argmax(y_pred_proba, axis=1)  # Konversi ke kelas prediksi (0, 1, 2, dst.)\n\ny_test_labels = y_test\n\n# Menghitung dan menampilkan classification report\nprint(classification_report(y_test_labels, y_pred, target_names=kelas))\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=kelas,\n            yticklabels=kelas)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()\n\n# %% [markdown] {\"jupyter\":{\"outputs_hidden\":false}}\n# **Save model (.h5)**\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-11-11T14:09:53.253221Z\",\"iopub.execute_input\":\"2024-11-11T14:09:53.253658Z\",\"iopub.status.idle\":\"2024-11-11T14:09:53.299851Z\",\"shell.execute_reply.started\":\"2024-11-11T14:09:53.253619Z\",\"shell.execute_reply\":\"2024-11-11T14:09:53.298893Z\"},\"jupyter\":{\"outputs_hidden\":false}}\nmodel.save(\"modelAudio.h5\")\n#size only 180KB\n\n# %% [markdown]\n# # Kesimpulan\n# \n# **Akurasi**\n# \n# `Random Forest`: Akurasi 0.96 atau 96%.\n# `Neural Network`: Akurasi 0.97 atau 97%.\n# Maka dari itu Neural Network memiliki akurasi sedikit lebih tinggi dibandingkan dengan Random Forest, menunjukkan bahwa model Neural Network lebih mampu menangkap pola dalam data ini.\n# \n# **Recall, F1-Score, Precision**\n# `Random Forest` menunjukkan performa baik namun sedikit lebih bervariasi di beberapa kelas (misalnya, pada kelas \"happy\" dan \"surprise\").\n# `Neural Network` memiliki performa lebih konsisten di berbagai kelas, dengan hampir semua kelas memiliki precision, recall, dan F1-score di atas 0.94, serta lebih tinggi pada kelas yang lebih sulit seperti \"surprise.\"\n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\n","metadata":{"_uuid":"ec4bb1bf-0a81-4bc3-b71f-5a2e6ac4eec1","_cell_guid":"fdff9d13-347a-4cad-b231-ff657f2e84ca","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}